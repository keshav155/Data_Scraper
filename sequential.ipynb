{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests , re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(\"C:/Users/Keshav Singh/Downloads/chromedriver_win32/chromedriver.exe\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_and_URL_fetcher(df , url):\n",
    "    driver.get(url)\n",
    "    host_website = 'https://www.calorieking.com'\n",
    "    more_buttons = driver.find_elements_by_xpath(\"//a[@role = 'button']\")\n",
    "\n",
    "    for x in range(len(more_buttons)):\n",
    "        if more_buttons[x].is_displayed():\n",
    "            driver.execute_script(\"arguments[0].click();\", more_buttons[x])\n",
    "            time.sleep(1)\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    soup = BeautifulSoup(page_source,'lxml')\n",
    "    food_name = []\n",
    "    links = []\n",
    "    \n",
    "    for url in soup.find_all(\"a\",class_ = \"MuiButtonBase-root MuiListItem-root MuiListItem-gutters MuiListItem-button\",href=True):\n",
    "        links.append(url['href'])\n",
    "    \n",
    "    for name in soup.find_all(\"span\",class_ = \"MuiTypography-root MuiListItemText-primary jss372 MuiTypography-body1 MuiTypography-noWrap MuiTypography-displayBlock\"):\n",
    "        food_name.append(name.text)\n",
    "        \n",
    "    final_urls = [host_website + x for x in links]    \n",
    "    df['Name'] = food_name\n",
    "    df['URL'] = final_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df = pd.DataFrame()\n",
    "url = \"https://www.calorieking.com/au/en/foods/b/calories-in-ben-jerrys/MxFEtDAgSWGXay3ra1qUlw?catId=4TmHVi3pQjma8PMa5jjDpQ\"\n",
    "name_and_URL_fetcher(restaurant_df , url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_be_inserted = []\n",
    "np.array(data_to_be_inserted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching Data for the url\n",
    "def data_fetcher(url):\n",
    "    my_url = url\n",
    "\n",
    "    source = requests.get(my_url).text\n",
    "\n",
    "    soup = BeautifulSoup(source,'lxml')\n",
    "\n",
    "    calories = soup.find('th',class_=\"MuiTableCell-root MuiTableCell-body jss465 MuiTableCell-sizeSmall\").text\n",
    "    calories = calories.split()\n",
    "    calories = calories[1] + \" \" +calories[0] \n",
    "    #print(calories)\n",
    "\n",
    "    kilojoule = soup.find('td',class_=\"MuiTableCell-root MuiTableCell-body MuiTableCell-alignRight MuiTableCell-sizeSmall\").text\n",
    "    kilojoule = kilojoule[1:-1]\n",
    "    #print(kilojoule)\n",
    "\n",
    "    protein = soup.find('th',text = 'Protein').parent.td.text\n",
    "    #print(protein)\n",
    "\n",
    "    total_fats = soup.find('th',text = 'Total Fat').parent.td.text\n",
    "    #print(total_fats)\n",
    "\n",
    "    saturated_fats = soup.find('th',text = 'Saturated Fat').parent.td.text\n",
    "    #print(saturated_fats)\n",
    "\n",
    "    carbohydrates = soup.find('th',text = 'Carbohydrate').parent.td.text\n",
    "    #print(carbohydrates)\n",
    "\n",
    "    sugars = soup.find('th',text = 'Sugars').parent.td.text\n",
    "    #print(sugars)\n",
    "\n",
    "    sodium = soup.find('th',text = 'Sodium').parent.td.text\n",
    "    #print(sodium)\n",
    "    data_to_insert = [ calories,kilojoule,protein,total_fats,saturated_fats,carbohydrates,sugars,sodium ]\n",
    "    return data_to_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching Data for the url\n",
    "def data_fetcher_for_meal(url):\n",
    "    my_url = url\n",
    "\n",
    "    source = requests.get(my_url).text\n",
    "\n",
    "    soup = BeautifulSoup(source,'lxml')\n",
    "\n",
    "    calories = soup.find('th',class_=\"MuiTableCell-root MuiTableCell-body jss466 MuiTableCell-sizeSmall\").text\n",
    "    calories = calories.split()\n",
    "    calories = calories[1] + \" \" +calories[0] \n",
    "    #print(calories)\n",
    "\n",
    "    kilojoule = soup.find('td',class_=\"MuiTableCell-root MuiTableCell-body MuiTableCell-alignRight MuiTableCell-sizeSmall\").text\n",
    "    kilojoule = kilojoule[1:-1]\n",
    "    #print(kilojoule)\n",
    "\n",
    "    protein = soup.find('th',text = 'Protein').parent.td.text\n",
    "    #print(protein)\n",
    "\n",
    "    total_fats = soup.find('th',text = 'Total Fat').parent.td.text\n",
    "    #print(total_fats)\n",
    "\n",
    "    saturated_fats = soup.find('th',text = 'Saturated Fat').parent.td.text\n",
    "    #print(saturated_fats)\n",
    "\n",
    "    carbohydrates = soup.find('th',text = 'Carbohydrate').parent.td.text\n",
    "    #print(carbohydrates)\n",
    "\n",
    "    sugars = soup.find('th',text = 'Sugars').parent.td.text\n",
    "    #print(sugars)\n",
    "\n",
    "    sodium = soup.find('th',text = 'Sodium').parent.td.text\n",
    "    #print(sodium)\n",
    "    data_to_insert = [ calories,kilojoule,protein,total_fats,saturated_fats,carbohydrates,sugars,sodium ]\n",
    "    return data_to_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 has been scraped successfully\n",
      "1 has been scraped successfully\n",
      "2 has been scraped successfully\n",
      "3 has been scraped successfully\n",
      "4 has been scraped successfully\n",
      "5 has been scraped successfully\n",
      "6 has been scraped successfully\n",
      "7 has been scraped successfully\n",
      "8 has been scraped successfully\n",
      "9 has been scraped successfully\n",
      "10 has been scraped successfully\n",
      "11 has been scraped successfully\n",
      "12 has been scraped successfully\n",
      "13 has been scraped successfully\n",
      "14 has been scraped successfully\n",
      "15 has been scraped successfully\n",
      "16 has been scraped successfully\n",
      "17 has been scraped successfully\n",
      "18 has been scraped successfully\n",
      "19 has been scraped successfully\n",
      "20 has been scraped successfully\n",
      "21 has been scraped successfully\n",
      "22 has been scraped successfully\n",
      "23 has been scraped successfully\n",
      "24 has been scraped successfully\n",
      "25 has been scraped successfully\n",
      "26 has been scraped successfully\n",
      "27 has been scraped successfully\n",
      "28 has been scraped successfully\n",
      "29 has been scraped successfully\n",
      "30 has been scraped successfully\n",
      "31 has been scraped successfully\n",
      "32 has been scraped successfully\n",
      "33 has been scraped successfully\n",
      "34 has been scraped successfully\n",
      "35 has been scraped successfully\n",
      "36 has been scraped successfully\n",
      "37 has been scraped successfully\n",
      "38 has been scraped successfully\n",
      "Finished in 33.1340077 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "# Organizing data for inputting\n",
    "for extract_data in range(len(restaurant_df.index)):\n",
    "    try: data_to_insert = data_fetcher(restaurant_df.URL[extract_data])         \n",
    "    except: data_to_insert = data_fetcher_for_meal(restaurant_df.URL[extract_data])\n",
    "    data_to_be_inserted.append(data_to_insert)\n",
    "    print(str(extract_data) + \" has been scraped successfully\")\n",
    "t2 = time.perf_counter()\n",
    "print(f'Finished in {t2-t1} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = [\"calories\",\"kilojoule\",\"protein\",\"total_fats\",\"saturated_fats\",\"carbohydrates\",\"sugars\",\"sodium\"]\n",
    "restaurant_df_nutrients = pd.DataFrame(columns=column_list)\n",
    "rows_list = []\n",
    "for row in data_to_be_inserted:\n",
    "    restaurant_df_nutrients.loc[len(restaurant_df_nutrients)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df_final = pd.concat([restaurant_df, restaurant_df_nutrients], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df_final.to_excel(\"data_ben&jerrys.xlsx\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
